{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPython 2.7 compatible\\nCommentary and portions of code Created by Andrew Strickland July 2018\\n\\nNotice:\\n- A plenty of modifications have been made to this workbook between the dates of 06/10/2018-08/15/2018\\n- The goal of this program is to lexically analyze research abstracts to form connections and statistical intuitions concerning the subject of\\n- honeybee colony colapse.\\n\\nMentions:\\n- This project is designed to export files that are interoperable with Gephi, and the gephi companion \"GraphBuilder.jar\" designed by Andrew Strickland\\n- For detailed instruction on how to use the gephi companion, see the README.txt file under /GraphBuilder/src/\\n- For any cell that has my name, Andrew Strickland, in its header, I am available to give reasonable aid to \\n- a reasonable misunderstanding. Refer limited questions to apstrick@uncg.edu\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import logging\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from datetime import datetime\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from collections import namedtuple\n",
    "import gensim.parsing.preprocessing as processing\n",
    "from os import listdir\n",
    "import CustomApi as api\n",
    "from scipy import spatial\n",
    "#Start Global Variables and Types\n",
    "\n",
    "#document dir\n",
    "docs = \"../data/documents/\"\n",
    "\n",
    "#Number of topics the model should index\n",
    "numberOfTopics = 150\n",
    "\n",
    "#Number of passes the model should make\n",
    "passes = 75\n",
    "\n",
    "#Keyphrase tracker\n",
    "keyphraseTracker = api.KeyWordTracker()\n",
    "\n",
    "#End Global Variables and Types\n",
    "\"\"\"\n",
    "Python 2.7 compatible\n",
    "Commentary and portions of code Created by Andrew Strickland July 2018\n",
    "\n",
    "Notice:\n",
    "- A plenty of modifications have been made to this workbook between the dates of 06/10/2018-08/15/2018\n",
    "- The goal of this program is to lexically analyze research abstracts to form connections and statistical intuitions concerning the subject of\n",
    "- honeybee colony colapse.\n",
    "\n",
    "Mentions:\n",
    "- This project is designed to export files that are interoperable with Gephi, and the gephi companion \"GraphBuilder.jar\" designed by Andrew Strickland\n",
    "- For detailed instruction on how to use the gephi companion, see the README.txt file under /GraphBuilder/src/\n",
    "- For any cell that has my name, Andrew Strickland, in its header, I am available to give reasonable aid to \n",
    "- a reasonable misunderstanding. Refer limited questions to apstrick@uncg.edu\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "Theses are the names of the csv files containing all of our data.\n",
    "\"\"\"\n",
    "files = listdir(docs+\"lt2006/\")\n",
    "files1 = listdir(docs+\"gt2006/\")\n",
    "entries = []\n",
    "\"\"\"\n",
    "Andrew Strickland\n",
    "Reads each csv file in and adds it to our list of data so long as it has and abstract title and isnt the legend row (aka Authors, Date, Abstract text... the very first row)\n",
    "\"\"\"\n",
    "for file in files:\n",
    "    with open(docs+\"lt2006/\"+file,\"rb\") as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for row in spamreader:\n",
    "            #If its not the first row and it has an abstract, read it in.\n",
    "            if \"Authors\" not in row[0] and row[15] !='[No abstract available]':\n",
    "                entries.append(row)\n",
    "for file in files1:\n",
    "    with open(docs+\"gt2006/\"+file,\"rb\") as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for row in spamreader:\n",
    "            if \"Authors\" not in row[0] and row[15] !='[No abstract available]':\n",
    "                entries.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Josh Moore\n",
    "Use synonyms to replace words. These sysnonyms are listed under the csv file cleanSynonym.csv\n",
    "\"\"\"\n",
    "\n",
    "syns = {}\n",
    "syn_topics = []\n",
    "with open(\"../data/cleanSynonyms.csv\",\"r\") as syn_file:\n",
    "    syn_file.readline()\n",
    "    for line in syn_file:\n",
    "        if len(line.split(\",\")[1])>1:\n",
    "            syn_topics.append(line.split(\",\")[1].strip())\n",
    "            syns[line.split(\",\")[0].strip()]=line.split(\",\")[1].strip()\n",
    "        else:\n",
    "            syn_topics.append(line.split(\",\")[0].strip())\n",
    "            \n",
    "def syn_replace(string):\n",
    "    temp = string\n",
    "    for word in syns:\n",
    "        temp = temp.replace(word,syns[word])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "Stems the text of the abstract and removes stop words. The abstracts are then grouped into \"selections\" where each selection\n",
    "is just a grouping of no less than 50 abstracts. These selections allow us to run a model over a small selection (usually results in being 1 pub. year)\n",
    "of abstracts and compare years and perform analysis on the trends and changes between years.\n",
    "\"\"\"\n",
    "# Create a set of frequent words\n",
    "stopFile = open(\"../data/stopwords.txt\",\"r\")\n",
    "stopWords = stopFile.read().splitlines() #This is the copyright symbol, this shows up in every abstract and should not be apart of the corpus\n",
    "stopWords.extend([u\"\\u2019\",u\"\\u03bc\",\"bee\",\"bees\",\"honey\",\"honeybee\",\"honeybees\",u\"\\xa9\",u\"\\xc2\"])\n",
    "# for asc in range(97,123):\n",
    "#     stopWords.extend([chr(asc)])\n",
    "with open(\"../data/extraStopWords.txt\",\"r\") as extraStopFile:\n",
    "    stopWords.extend(extraStopFile.read().split(\"\\n\"))\n",
    "# Lowercase each document, split it by white space and filter out stopWords\n",
    "stopList = set(stopWords)\n",
    "processing.STOPWORDS = stopList\n",
    "ps = PorterStemmer()\n",
    "def removeStops(text):\n",
    "    stopsRemoved = processing.remove_stopwords(text.lower().translate(None, string.punctuation))\n",
    "    words = stopsRemoved.split(\" \")\n",
    "    stemmedWords = []\n",
    "    for w in words:\n",
    "        if len(ps.stem(w)) > 2:\n",
    "            stemmedWords.append(ps.stem(w))\n",
    "    return ' '.join(stemmedWords)\n",
    "\n",
    "\n",
    "\n",
    "# Each abstract has a 'title':String, 'date':datetime.datetime, 'text':String, and 'keywords':String\n",
    "abstracts = [api.MyAbstract._make([art[1],datetime.strptime(art[2], '%Y'),syn_replace(removeStops(art[15])), syn_replace(art[16])]) for art in entries]\n",
    "abstracts.sort(key=lambda q: q.date.year)\n",
    "entries = None\n",
    "# Count word frequencies\n",
    "selections = []\n",
    "\n",
    "access = lambda x: x.date.year\n",
    "lastIndex = 0\n",
    "bettersel = {}\n",
    "for i in range(1957,2018):\n",
    "    index = api.binarySearch(abstracts,i,access)\n",
    "    if  index != -1:\n",
    "        selections.append(abstracts[lastIndex:index+1])\n",
    "        lastIndex = index+1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9745582342147827\n",
      "0.9734410047531128\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Josh Moore\n",
    "Creates the topic vectors for the corpus and \n",
    "\"\"\"\n",
    "\n",
    "x = \"\"\n",
    "for i in selections:\n",
    "    x+=i[0][2]\n",
    "d = corpora.Dictionary([x.split(\" \")])\n",
    "common_corpus = [d.doc2bow(x.split(\" \")) for text in x]\n",
    "\n",
    "lda = models.ldamodel.LdaModel(common_corpus, num_topics=numberOfTopics, minimum_probability=0.0)\n",
    "\n",
    "corpi = {}\n",
    "count = 0\n",
    "for x in selections:\n",
    "    corpi[count] = \"\"\n",
    "    for y in x:\n",
    "        corpi[count]+=y[2]+\" \"\n",
    "    count+=1\n",
    "\n",
    "# Divides the corpus into three groups based on year\n",
    "\n",
    "split = []\n",
    "temp = \"\"\n",
    "for x in range(len(corpi)-6,len(corpi)):\n",
    "    temp+=corpi[x]\n",
    "split.append(temp)\n",
    "\n",
    "temp = \"\"\n",
    "for x in range(len(corpi)-14,len(corpi)-6):\n",
    "    temp+=corpi[x]\n",
    "split.append(temp)\n",
    "\n",
    "temp = \"\"\n",
    "for x in range(0,len(corpi)-14):\n",
    "    temp+=corpi[x]\n",
    "split.append(temp)\n",
    "\n",
    "vectors = []\n",
    "for key in split:\n",
    "    cur_corpus = [[d.doc2bow(key.split(\" \"))]]\n",
    "    for x in lda[cur_corpus[0]]:\n",
    "        vectors.append(x)\n",
    "\n",
    "just_nums = [[x[1] for x in vectors[c]]for c in range(0,len(vectors))]\n",
    "\n",
    "for x in range(0,len(just_nums)-1):\n",
    "    print (1 - spatial.distance.cosine(just_nums[x], just_nums[x+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "def createCorpus(selection):\n",
    "    frequency = defaultdict(int)\n",
    "    for abst in abstracts:\n",
    "        for token in abst.text.split(\" \"):\n",
    "            frequency[token] += 1\n",
    "    tempFolder = tempfile.gettempdir()\n",
    "    processedCorpus = [[token for token in abst.text.split(\" \") if frequency[token] > 5] for abst in selection]\n",
    "    dictionary = corpora.Dictionary(processedCorpus)\n",
    "    dictionary.save(os.path.join(tempFolder,'words.dict'))\n",
    "    # Create general corpus and serialize in order for it to be iterated over\n",
    "    corpus = [dictionary.doc2bow(text) for text in processedCorpus]\n",
    "    corpora.MmCorpus.serialize(os.path.join(tempFolder, 'words.dict'), corpus)\n",
    "    return api.MyCorpora._make([corpus,dictionary])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "\"\"\"\n",
    "numTopicsExport = 50\n",
    "def exportResults(path, sortedVals,wordDict):\n",
    "    seg = sortedVals[0][0:numTopicsExport]\n",
    "    #Edges and their weights [(a,b):weight]\n",
    "    edges = sortedVals[1]\n",
    "    # set of (a,b)\n",
    "    exportEdges = set({})\n",
    "    exportNodes = set({})\n",
    "    for node in seg:\n",
    "        exportNodes.add((node[\"tag\"],node[\"word\"],node[\"occurences\"]))\n",
    "        for e in node[\"edges\"]:\n",
    "            a = wordDict[e[0]]\n",
    "            b = wordDict[e[1]]\n",
    "            if a in seg and b in seg:\n",
    "                exportEdges.add(e)\n",
    "                exportNodes.add((a[\"tag\"],a[\"word\"],a[\"occurences\"]))\n",
    "                exportNodes.add((b[\"tag\"],b[\"word\"],b[\"occurences\"]))\n",
    "            \n",
    "    with open(path+\"nodes.csv\",\"w\") as file:\n",
    "        file.write(\"Id,Label,Weight\\n\")\n",
    "        for val in exportNodes:\n",
    "            file.write(str(val[0])+\",\"+val[1].encode('utf8')+','+str(val[2])+'\\n')\n",
    "    with open(path+'edges.csv',\"w\") as file:\n",
    "        file.write(\"Source,Target,Weight\\n\")\n",
    "        #We dont want to have duplicate edges written, so we want to make an aedge buffer and then write the buffer\n",
    "        for e in exportEdges:\n",
    "            file.write(str(e[0])+\",\"+str(e[1])+\",\"+str(edges[e])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "\"\"\"\n",
    "import math as m\n",
    "def getResults(topics):\n",
    "    mapper = api.WordMapper()\n",
    "    #Edges [(a,b):weight]\n",
    "    edges = {}\n",
    "    def updateEdgeFreq(vec1):\n",
    "        if vec1[0] == vec1[1]:\n",
    "            return None\n",
    "        altVec = [vec1[1],vec1[0]]\n",
    "        if vec1 in edges.keys():\n",
    "            edges[vec1] += 1\n",
    "            return vec1\n",
    "        elif altVec in edges.keys():\n",
    "            edges[altVec] += 1\n",
    "            return altVec\n",
    "        else:\n",
    "            edges[vec1] = 1\n",
    "            return vec1\n",
    "    occurences = {}\n",
    "    topics[len(topics)-1]\n",
    "    for topic in topics:\n",
    "        words = topic[0]\n",
    "        #make all the edges for this topic\n",
    "        wordEdges = []\n",
    "        wid = mapper.mapWord(words[0][1])\n",
    "        for t in words:\n",
    "            edge = updateEdgeFreq((wid,mapper.mapWord(t[1])))\n",
    "            if edge != None:\n",
    "                wordEdges.append(edge)\n",
    "        #add the edges to each word and and update the number of occurences per word\n",
    "        for w in words:\n",
    "            word = w[1]\n",
    "            wid = mapper.mapWord(word)\n",
    "            if wid not in occurences.keys():\n",
    "                occurences[wid] = {\"prob\":w[0],\"occurences\":1,\"edges\":wordEdges, \"tag\" : wid, \"word\":word}\n",
    "            else:\n",
    "                occurences[wid][\"occurences\"] += 1\n",
    "                occurences[wid][\"prob\"] = max(occurences[wid][\"prob\"], w[0])\n",
    "    return [occurences,edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "Andrew Strickland\n",
    "\"\"\"\n",
    "from difflib import SequenceMatcher\n",
    "size = 0\n",
    "start = 0\n",
    "import os\n",
    "occurences = None\n",
    "try:\n",
    "    for i in range(0,len(selections)):\n",
    "        #keyphraseTracker = api.KeyWordTracker()\n",
    "        size += len(selections[i])\n",
    "        if size >=50:\n",
    "            selection = api.flaten(selections[start:i+1])\n",
    "            for abstr in selection:\n",
    "                phrases = [k.strip() for k in  abstr.keywords.split(\";\")]\n",
    "                keyPhrases = []\n",
    "                for phr in phrases:\n",
    "                    words = [ps.stem(w) if all(ord(c) < 128 for c in w) else None for w in phr.split(\" \")]\n",
    "                    string = \"\"\n",
    "                    for w in words:\n",
    "                        if w != None:\n",
    "                            string = string +\" \"+ w\n",
    "                    string = string.strip()\n",
    "                    if len(string) > 2:\n",
    "                        keyPhrases.append(string)\n",
    "                for kph in keyPhrases:    \n",
    "                    keyphraseTracker.track(kph,abstr.date.year)\n",
    "                    keyphraseTracker.registerCoOccurrences(kph,keyPhrases)\n",
    "            #continue to next section\n",
    "            corpus = createCorpus(selection)\n",
    "            path = docs+'GephiFiles/'+str(selection[0].date.year)+'/'\n",
    "            \n",
    "            # Make this TFIDF\n",
    "            tfidf = TfidfModel(corpus.corpus,id2word=corpus.id2word)\n",
    "            \n",
    "            \n",
    "            logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "            \n",
    "            \n",
    "#             lda = models.ldamodel.LdaModel(corpus.corpus,id2word=corpus.id2word,num_topics=numberOfTopics,passes=passes)\n",
    "            lda = models.ldamodel.LdaModel(tfidf[corpus.corpus],id2word=corpus.id2word,num_topics=numberOfTopics,passes=passes)\n",
    "            \n",
    "            topics = lda.top_topics(corpus=corpus.corpus,dictionary=corpus.id2word,topn=3)\n",
    "            \n",
    "            topics.sort(key=lambda k:k[1],reverse=True)\n",
    "            occurences = getResults(topics)\n",
    "            sortedVals = [sorted(occurences[0].values(),key=lambda k:k[\"occurences\"],reverse=True),occurences[1]]\n",
    "            #####Release some of this memory please and thank you\n",
    "            selection = None\n",
    "            #occurences = None\n",
    "            #####\n",
    "            exportResults(path,sortedVals,occurences[0])\n",
    "            size = 0\n",
    "            start = i+1\n",
    "except Exception as e:\n",
    "    #this just sends me an email when the process fails.\n",
    "    os.system(\"emailme \\\"Failed come check it out: \"+str(e)+\" \\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "Tracks the usage of words over the entire corpus. For detailed functionality see the KeyWordTracker class in \n",
    "the CustomApi.py file. Registers the co-occurrence of each phrase with the other phrases in the abstract description.\n",
    "\"\"\"\n",
    "keyphraseTracker = api.KeyWordTracker()\n",
    "for abstr in abstracts:\n",
    "    phrases = [k.strip() for k in  abstr.keywords.split(\";\")]\n",
    "    for kph in phrases:    \n",
    "        keyphraseTracker.track(kph,abstr.date.year)\n",
    "        keyphraseTracker.registerCoOccurrences(kph,phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "Partitions the top 50 keywords in the entire corpus to be exported. These are not new keywords, just those that\n",
    "appear the most over the course of all the documents.\n",
    "\"\"\"\n",
    "\n",
    "top50 = keyphraseTracker.topN()\n",
    "keyphraseNodes = {}\n",
    "for yo in top50:\n",
    "    keyphraseNodes[yo.getPhrase()] = yo.sum()\n",
    "keyphraseEdges = []\n",
    "for yo in top50:\n",
    "    for (phr,occur) in yo.cooccurringPhrases.iteritems():\n",
    "        #rint (phr,occur)\n",
    "        if phr in keyphraseNodes.keys():\n",
    "            keyphraseEdges.append((yo.getPhrase(),phr,occur))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print len(keyphraseNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "This cell splits the keywords from each abstract into three groups. 1974-2004,2005-2012,2013-2018\n",
    "\"\"\"\n",
    "pre04 = []\n",
    "post04 = []\n",
    "post12 = []\n",
    "notfirst89 = []\n",
    "notfirst04 = []\n",
    "notfirst18 = []\n",
    "yearly = {}\n",
    "allKeys = {}\n",
    "for word in keyphraseTracker.words.values():\n",
    "    allKeys[word.phrase.decode('utf-8')]=word.sum()\n",
    "    for key in word.years:\n",
    "        if key <= 2004 and word not in notfirst89:\n",
    "            notfirst89.append(word)\n",
    "        elif key <= 2012 and key > 2004 and word not in notfirst04:\n",
    "            notfirst04.append(word)\n",
    "        elif key > 2012 and word not in notfirst18:\n",
    "            notfirst18.append(word)\n",
    "    if any(a < 2004 for a in word.years):\n",
    "        pre04.append(word)\n",
    "    if any(a <= 2012 and a>=2004 for a in word.years):\n",
    "        post04.append(word)\n",
    "    if any(a > 2012 for a in word.years):\n",
    "        post12.append(word)\n",
    "pre04.sort(key=lambda x:x.sum(),reverse=True)\n",
    "post04.sort(key=lambda x:x.sum(),reverse=True)\n",
    "post12.sort(key=lambda x:x.sum(),reverse=True)\n",
    "notfirst89.sort(key=lambda x:x.sum(),reverse=True)\n",
    "notfirst04.sort(key=lambda x:x.sum(),reverse=True)\n",
    "notfirst18.sort(key=lambda x:x.sum(),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/KeywordOccurance.tsv\",\"w\") as f:\n",
    "    f.write(\"Key\\tValue\\n\")\n",
    "    for key in allKeys:\n",
    "        f.write(key.encode(\"utf-16\")[2:]+\"\\t\"+str(allKeys[key])+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "This cell builds the node and edge file for the keywords split up into three groups.\n",
    "1974-2004,2005-2012,2013-2018\n",
    "\"\"\"\n",
    "keyphraseMapper = api.WordMapper()\n",
    "def writeNodes(path, arr):\n",
    "    with open(path,\"w\") as nodeFile:\n",
    "        nodeFile.write(\"Id,Label,Weight\\n\")\n",
    "        for w in arr[0:50]:\n",
    "            id = keyphraseMapper.mapWord(w.phrase)\n",
    "            nodeFile.write(str(id)+\",\"+w.phrase+\",\"+str(w.sum())+\"\\n\")\n",
    "writeNodes(\"../data/documents/GephiFiles/new89/nodes.csv\",pre04)\n",
    "writeNodes(\"../data/documents/GephiFiles/new04/nodes.csv\",post04)\n",
    "writeNodes(\"../data/documents/GephiFiles/new18/nodes.csv\",post12)\n",
    "def writeEdges(path,arr):\n",
    "    keyphraseEdges1 = []\n",
    "    for yo in arr[0:50]:\n",
    "        for (phr,occur) in yo.cooccurringPhrases.iteritems():\n",
    "            edgeWord = keyphraseTracker.words[phr]\n",
    "            if edgeWord in arr[0:50]:\n",
    "                id = keyphraseMapper.mapWord(yo.getPhrase())\n",
    "                id2 = keyphraseMapper.mapWord(phr)\n",
    "                keyphraseEdges1.append((id,id2,occur))\n",
    "    with open(path,\"w\") as edgeFile:\n",
    "        edgeFile.write(\"Source,Destination,Weight\\n\")\n",
    "        for (id1,id2,w) in keyphraseEdges1:\n",
    "            edgeFile.write(str(id1)+\",\"+str(id2)+\",\"+str(w)+\"\\n\")\n",
    "writeEdges(\"../data/documents/GephiFiles/new89/edges.csv\",pre04)\n",
    "writeEdges(\"../data/documents/GephiFiles/new04/edges.csv\",post04)\n",
    "writeEdges(\"../data/documents/GephiFiles/new18/edges.csv\",post12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Andrew Strickland\n",
    "\"\"\"\n",
    "with open(\"../data/documents/GephiFiles/KeyWords/nodes.csv\",\"w\") as nodesFile:\n",
    "    nodesFile.write(\"Id,Label,Weight\\n\")\n",
    "    for (phr,occur) in keyphraseNodes.iteritems():\n",
    "        id = keyphraseMapper.mapWord(phr)\n",
    "        nodesFile.write(str(id)+\",\"+phr+\",\"+str(occur)+\"\\n\")\n",
    "mappedEdges = [(keyphraseMapper.mapWord(t[0]),keyphraseMapper.mapWord(t[1]),t[2]) for t in keyphraseEdges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Josh Moore\n",
    "This function returns the edges for a given set of topics\n",
    "\"\"\"\n",
    "def give_Edges(arr):\n",
    "    translate89 = {}\n",
    "    for phrase in arr[:150]:\n",
    "        for p in phrase.cooccurringPhrases.iteritems():\n",
    "            translate89[str(keyphraseMapper.mapWord(p[0]))]=p[0]\n",
    "    #         print p[0]+\" \"+str(keyphraseMapper.mapWord(p[0]))\n",
    "    edges89 = []\n",
    "    for node in arr[:150]:\n",
    "        for (phr,occur) in node.cooccurringPhrases.iteritems():\n",
    "                edgeWord = keyphraseTracker.words[phr]\n",
    "                if edgeWord in arr[:150]:\n",
    "                    id = node.getPhrase()\n",
    "                    id2 = translate89[str(keyphraseMapper.mapWord(phr))]\n",
    "                    edges89.append((id,id2,occur))\n",
    "    return edges89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Josh Moore\n",
    "This returns the network figures that connects nodes based on how many appereances the keywords make in the corpus\n",
    "\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "plt.figure(figsize=(15,15))\n",
    "current_set = post12\n",
    "\n",
    "nodes = []\n",
    "weights = []\n",
    "f = open(\"../data/post12top10.csv\",\"w\")\n",
    "count = 0\n",
    "while len(nodes)<11:\n",
    "    phrase = current_set[count]\n",
    "    if not any(x.lower() in phrase.getPhrase().lower() for x in syns):\n",
    "        if u'\\u03b1' not in phrase.phrase.decode('utf-8').strip():\n",
    "            f.write(phrase.phrase.decode('utf-8').strip()+\",\"+str(phrase.sum())+\"\\n\")\n",
    "            nodes.append(phrase.phrase.decode('utf-8').strip())\n",
    "            weights.append(phrase.sum()*10)\n",
    "    count+=1\n",
    "f.close()\n",
    "g = nx.Graph()\n",
    "\n",
    "g.add_nodes_from(nodes)\n",
    "pos = nx.spring_layout(g)\n",
    "temp = give_Edges(current_set)\n",
    "toDel = []\n",
    "for x in range(0,len(temp)):\n",
    "#     print temp[x]\n",
    "    if not temp[x][0] in nodes or not temp[x][1] in nodes :\n",
    "        toDel.append(x)\n",
    "for x in reversed(toDel):\n",
    "    del temp[x]\n",
    "temp.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "g.add_weighted_edges_from(temp)\n",
    "\n",
    "d = nx.degree(g)\n",
    "labels = {x:x for x in nodes}\n",
    "\n",
    "nx.draw_networkx_labels(g,pos,labels,font_size=14)\n",
    "plt.axis('off')\n",
    "nx.draw(g,pos,node_size=weights,edge_color='blue')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Josh Moore\n",
    "Creates files for first occurence per year figures\n",
    "\"\"\"\n",
    "arr = sorted(keyphraseTracker.words.values(),key=lambda k:k.sum(),reverse=True)\n",
    "first_year = {}\n",
    "for a in arr:\n",
    "    if a.firstOccurence() not in first_year:\n",
    "        first_year[a.firstOccurence()] = []\n",
    "    first_year[a.firstOccurence()].append(a)\n",
    "for x in first_year:\n",
    "    first_year[x].sort(key=lambda k:k.sum())\n",
    "for x in first_year:\n",
    "    with open(\"../figures/\"+str(x)+\"firstOccurence.csv\",\"w\") as f:\n",
    "        for y in first_year[x][:25]:\n",
    "            f.write(y.getPhrase()+\",\"+str(y.sum())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
