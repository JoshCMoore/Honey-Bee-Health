{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import logging\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from datetime import datetime\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import namedtuple\n",
    "import gensim.parsing.preprocessing as processing\n",
    "from os import listdir\n",
    "import CustomApi as api\n",
    "#Start Global Variables and Types\n",
    "\n",
    "#document dir\n",
    "docs = \"../data/documents/\"\n",
    "\n",
    "#Number of topics the model should index\n",
    "numberOfTopics = 225\n",
    "\n",
    "#Number of passes the model should make\n",
    "passes = 75\n",
    "\n",
    "#Keyphrase tracker\n",
    "keyphraseTracker = api.KeyWordTracker()\n",
    "\n",
    "#End Global Variables and Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = listdir(docs+\"lt2006/\")\n",
    "entries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(docs+\"lt2006/\"+file,\"rb\") as csvfile:\n",
    "        spamreader = csv.reader(csvfile)\n",
    "        for row in spamreader:\n",
    "            if \"Authors\" not in row[0] and row[15] !='[No abstract available]':\n",
    "                entries.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evidence of the Juvenile Hormone Methyl(2E,6E)-10,11-epoxy-3,7,11-trimethyl-2,6-dodecadienoate(JH-3) in Insects of Four Orders'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title of an abstract\n",
    "entries[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of frequent words\n",
    "stopFile = open(\"../data/stopwords.txt\",\"r\")\n",
    "stopWords = stopFile.read().splitlines() #This is the copyright symbol, this shows up in every abstract and should not be apart of the corpus\n",
    "stopWords.extend([u\"\\u2019\",u\"\\u03bc\",\"bee\",\"bees\",\"honey\",\"honeybee\",\"honeybees\",u\"\\xa9\",u\"\\xc2\"])\n",
    "# for asc in range(97,123):\n",
    "#     stopWords.extend([chr(asc)])\n",
    "with open(\"../data/extraStopWords.txt\",\"r\") as extraStopFile:\n",
    "    stopWords.extend(extraStopFile.read().split(\"\\n\"))\n",
    "# Lowercase each document, split it by white space and filter out stopWords\n",
    "stopList = set(stopWords)\n",
    "processing.STOPWORDS = stopList\n",
    "ps = PorterStemmer()\n",
    "def removeStops(text):\n",
    "    stopsRemoved = processing.remove_stopwords(text.lower().translate(None, string.punctuation))\n",
    "    words = stopsRemoved.split(\" \")\n",
    "    stemmedWords = []\n",
    "    for w in words:\n",
    "        if len(ps.stem(w)) > 2:\n",
    "            stemmedWords.append(ps.stem(w))\n",
    "    return ' '.join(stemmedWords)\n",
    "# Each abstract has a 'title':String, 'date':datetime.datetime, 'text':String, and 'keywords':String\n",
    "abstracts = [api.MyAbstract._make([art[1],datetime.strptime(art[2], '%Y'),removeStops(art[15]), art[16]]) for art in entries]\n",
    "abstracts.sort(key=lambda q: q.date.year)\n",
    "entries = None\n",
    "# Count word frequencies\n",
    "selections = []\n",
    "\n",
    "access = lambda x: x.date.year\n",
    "lastIndex = 0\n",
    "for i in range(1957,2007):\n",
    "    index = api.binarySearch(abstracts,i,access)\n",
    "    if  index != -1:\n",
    "        selections.append(abstracts[lastIndex:index+1])\n",
    "        lastIndex = index+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def createCorpus(selection):\n",
    "    frequency = defaultdict(int)\n",
    "    for abst in abstracts:\n",
    "        for token in abst.text.split(\" \"):\n",
    "            frequency[token] += 1\n",
    "    tempFolder = tempfile.gettempdir()\n",
    "    processedCorpus = [[token for token in abst.text.split(\" \") if frequency[token] > 5] for abst in selection]\n",
    "    dictionary = corpora.Dictionary(processedCorpus)\n",
    "    dictionary.save(os.path.join(tempFolder,'words.dict'))\n",
    "    # Create general corpus and serialize in order for it to be iterated over\n",
    "    corpus = [dictionary.doc2bow(text) for text in processedCorpus]\n",
    "    corpora.MmCorpus.serialize(os.path.join(tempFolder, 'words.dict'), corpus)\n",
    "    return api.MyCorpora._make([corpus,dictionary])\n",
    "    \n",
    "\n",
    "\n",
    "# Save the dictionary of tokens\n",
    "# def createModel():\n",
    "#     tempFolder = tempfile.gettempdir()\n",
    "#     dictionary = corpora.Dictionary(processedCorpusFor(selection))\n",
    "#     dictionary.save(os.path.join(tempFolder,'words.dict'))\n",
    "#     # Create general corpus and serialize in order for it to be iterated over\n",
    "#     corpus = [dictionary.doc2bow(text) for text in processedCorpus]\n",
    "#     corpora.MmCorpus.serialize(os.path.join(tempFolder, 'words.dict'), corpus)\n",
    "    # Train the model and set number of topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopicsExport = 50\n",
    "\n",
    "def exportResults(path, sortedVals,wordDict):\n",
    "    seg = sortedVals[0][0:numTopicsExport]\n",
    "    #Edges and their weights [(a,b):weight]\n",
    "    edges = sortedVals[1]\n",
    "    # set of (a,b)\n",
    "    exportEdges = set({})\n",
    "    exportNodes = set({})\n",
    "    for node in seg:\n",
    "        exportNodes.add((node[\"tag\"],node[\"word\"],node[\"occurences\"]))\n",
    "        for e in node[\"edges\"]:\n",
    "            a = wordDict[e[0]]\n",
    "            b = wordDict[e[1]]\n",
    "            if a in seg and b in seg:\n",
    "                exportEdges.add(e)\n",
    "                exportNodes.add((a[\"tag\"],a[\"word\"],a[\"occurences\"]))\n",
    "                exportNodes.add((b[\"tag\"],b[\"word\"],b[\"occurences\"]))\n",
    "            \n",
    "    with open(path+\"nodes.csv\",\"w\") as file:\n",
    "        file.write(\"Id,Label,Weight\\n\")\n",
    "        for val in exportNodes:\n",
    "            file.write(str(val[0])+\",\"+val[1].encode('utf8')+','+str(val[2])+'\\n')\n",
    "    with open(path+'edges.csv',\"w\") as file:\n",
    "        file.write(\"Source,Target,Weight\\n\")\n",
    "        #We dont want to have duplicate edges written, so we want to make an aedge buffer and then write the buffer\n",
    "        for e in exportEdges:\n",
    "            file.write(str(e[0])+\",\"+str(e[1])+\",\"+str(edges[e])+\"\\n\")\n",
    "#         edgesToWrite = set([])\n",
    "#         for val in seg:\n",
    "#             #Edges a node has\n",
    "#             for edge in val[\"edges\"]:\n",
    "#                 #Check if (a,b) is an edge else if (b,a) is an edge. The graph is undirected so we dont want duplicates\n",
    "#                 if edge in edges.keys():\n",
    "#                     #Edge wasnt in the buffer, add it\n",
    "#                     if edge not in edgesToWrite:\n",
    "#                         edgesToWrite.add(edge)\n",
    "#                 #(b,a) in edge list?\n",
    "#                 elif (edge[1],edge[0]) in edges.keys():\n",
    "#                     #edge wasnt in the buffer\n",
    "#                     edgesToWrite.add(edge)\n",
    "#         for edge in edgesToWrite:\n",
    "#             file.write(str(edge[0])+\",\"+str(edge[1])+\",\"+str(edges[edge])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "def getResults(topics):\n",
    "    mapper = api.WordMapper()\n",
    "    #Edges [(a,b):weight]\n",
    "    edges = {}\n",
    "    def updateEdgeFreq(vec1):\n",
    "        if vec1[0] == vec1[1]:\n",
    "            return None\n",
    "        altVec = [vec1[1],vec1[0]]\n",
    "        if vec1 in edges.keys():\n",
    "            edges[vec1] += 1\n",
    "            return vec1\n",
    "        elif altVec in edges.keys():\n",
    "            edges[altVec] += 1\n",
    "            return altVec\n",
    "        else:\n",
    "            edges[vec1] = 1\n",
    "            return vec1\n",
    "    occurences = {}\n",
    "    topics[len(topics)-1]\n",
    "    for topic in topics:\n",
    "        words = topic[0]\n",
    "        #make all the edges for this topic\n",
    "        wordEdges = []\n",
    "        wid = mapper.mapWord(words[0][1])\n",
    "        for t in words:\n",
    "            edge = updateEdgeFreq((wid,mapper.mapWord(t[1])))\n",
    "            if edge != None:\n",
    "                wordEdges.append(edge)\n",
    "        #add the edges to each word and and update the number of occurences per word\n",
    "        for w in words:\n",
    "            word = w[1]\n",
    "            wid = mapper.mapWord(word)\n",
    "            if wid not in occurences.keys():\n",
    "                occurences[wid] = {\"prob\":w[0],\"occurences\":1,\"edges\":wordEdges, \"tag\" : wid, \"word\":word}\n",
    "            else:\n",
    "                occurences[wid][\"occurences\"] += 1\n",
    "                occurences[wid][\"prob\"] = max(occurences[wid][\"prob\"], w[0])\n",
    "    return [occurences,edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0e6490a06a07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%(asctime)s : %(levelname)s : %(message)s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumberOfTopics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mtopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreallen\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_no\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meval_every\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36mlog_perplexity\u001b[1;34m(self, chunk, total_docs)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mperwordbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         logger.info(\n\u001b[0;32m    581\u001b[0m             \u001b[1;34m\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36mbound\u001b[1;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[1;31m# E[log p(doc | theta, beta)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElogthetad\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[1;31m# E[log p(theta | alpha) - log q(theta | gamma)]; assumes alpha is a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\numpy\\core\\fromnumeric.pyc\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   1867\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keepdims'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1869\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1870\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m             \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\USER\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\models\\ldamodel.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((id, cnt))\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[1;31m# E[log p(doc | theta, beta)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mElogthetad\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[1;31m# E[log p(theta | alpha) - log q(theta | gamma)]; assumes alpha is a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from difflib import SequenceMatcher\n",
    "size = 0\n",
    "start = 0\n",
    "import os\n",
    "occurences = None\n",
    "try:\n",
    "    for i in range(0,len(selections)):\n",
    "        #keyphraseTracker = api.KeyWordTracker()\n",
    "        size += len(selections[i])\n",
    "        if size >=50:\n",
    "            selection = api.flaten(selections[start:i+1])\n",
    "            for abstr in selection:\n",
    "                phrases = [k.strip() for k in  abstr.keywords.split(\";\")]\n",
    "                keyPhrases = []\n",
    "                for phr in phrases:\n",
    "                    words = [ps.stem(w) if all(ord(c) < 128 for c in w) else None for w in phr.split(\" \")]\n",
    "                    string = \"\"\n",
    "                    for w in words:\n",
    "                        if w != None:\n",
    "                            string = string +\" \"+ w\n",
    "                    string = string.strip()\n",
    "                    if len(string) > 2:\n",
    "                        keyPhrases.append(string)\n",
    "                for kph in keyPhrases:    \n",
    "                    keyphraseTracker.track(kph,abstr.date.year)\n",
    "                    keyphraseTracker.registerCoOccurrences(kph,keyPhrases)\n",
    "            #continue to next section\n",
    "            corpus = createCorpus(selection)\n",
    "            path = docs+'GephiFiles/'+str(selection[0].date.year)+'/'\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "                print \"made dirs\"\n",
    "            except:\n",
    "                #Already exists\n",
    "                pass\n",
    "            logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "            lda = models.ldamodel.LdaModel(corpus.corpus,id2word=corpus.id2word,num_topics=numberOfTopics,passes=passes)\n",
    "            topics = lda.top_topics(corpus=corpus.corpus,dictionary=corpus.id2word,topn=3)\n",
    "            \n",
    "            topics.sort(key=lambda k:k[1],reverse=True)\n",
    "            occurences = getResults(topics)\n",
    "            sortedVals = [sorted(occurences[0].values(),key=lambda k:k[\"occurences\"],reverse=True),occurences[1]]\n",
    "            #####Release some of this memory please and thank you\n",
    "            selection = None\n",
    "            #occurences = None\n",
    "            #####\n",
    "            exportResults(path,sortedVals,occurences[0])\n",
    "            size = 0\n",
    "            start = i+1\n",
    "    os.system(\"emailme \\\"Finished successfully!\\\"\")\n",
    "except Exception as e:\n",
    "    os.system(\"emailme \\\"Failed come check it out: \"+str(e)+\" \\\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above corpus shows the amount of times every word used in the documents is used in every indevidual document. Every word is represented by a token ID, the list of which can be found in \"words.dict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-faefbcd59663>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtopicOrganizingFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../data/topicorganization.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstracts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabstracts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdocTopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordTopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphiValues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtopicOrganizingFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myearOfAbstract\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtitleOfAbstract\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort the most interesting words per topic per document\n",
    "# This cell does not need to be run if only trying to create Top Nine terms per paper\n",
    "topicOrganizingFile = open(\"../data/topicorganization.tsv\",\"w\")\n",
    "for x in xrange(0,len(abstracts)):\n",
    "    doc = dictionary.doc2bow(abstracts[x].split())\n",
    "    docTopics, wordTopics, phiValues = lda.get_document_topics(doc, per_word_topics=True)\n",
    "    topicOrganizingFile.write(yearOfAbstract[x]+\"\\t\"+titleOfAbstract[x]+\"\\t\")\n",
    "    for y in xrange(0,min(3,len(docTopics))):\n",
    "        topicnumber = docTopics[y][0]\n",
    "        topicOrganizingFile.write(str(lda.show_topic(topicnumber))+\"\\t\")\n",
    "        #Sorts the word topics in decending order based on their greatest phi value\n",
    "        for z in xrange(0,len(phiValues)):\n",
    "            phiValues[z][1].sort(key=lambda q:q[1],reverse=True)\n",
    "        phiValues.sort(key=lambda q:q[1][0][1],reverse=True)\n",
    "        curindex=0\n",
    "        topwords = \"\"\n",
    "        for z in xrange(0,3):\n",
    "            while curindex<len(phiValues) and phiValues[curindex][1][0][0]!=topicnumber:\n",
    "                curindex+=1\n",
    "            if(curindex>=len(phiValues)):break\n",
    "            print len(phiValues)\n",
    "            print dictionary[phiValues[curindex][0]]\n",
    "            topwords+=str(dictionary[phiValues[curindex][0]].encode('utf-8').strip())+\" \"\n",
    "            curindex+=1\n",
    "        filter(lambda a:a[0]!=topicnumber,phiValues)\n",
    "        topicOrganizingFile.write(topwords+\"\\t\")\n",
    "    topicOrganizingFile.write(\"\\n\")\n",
    "topicOrganizingFile.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicWords = []\n",
    "for i in range(0,numberOfTopics):\n",
    "    t = lda.get_topic_terms(i,50)\n",
    "    currentWordList = []\n",
    "    for x in t:\n",
    "        word = str(dictionary[x[0]])\n",
    "        if word not in currentWordList:\n",
    "            currentWordList.append(word)\n",
    "    topicWords.append(currentWordList)\n",
    "topicListFile = open(\"../data/TopicWords/List-\"+str(numberOfTopics)+\".txt\",\"w+\")\n",
    "for i in range(0,len(topicWords)):\n",
    "    topicListFile.write(\"Topic \"+str(i)+\":\\n\")\n",
    "    for j in topicWords[i]:\n",
    "        topicListFile.write(j+'\\n')\n",
    "    topicListFile.write('\\n')\n",
    "topicListFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes the top nine terms for each document\n",
    "\n",
    "topNineFile = open(\"../data/Docbow/TopNineTerms-\"+str(numberOfTopics)+\".tsv\",\"w\")\n",
    "for abstr in abstracts:\n",
    "    doc = dictionary.doc2bow(abstracts[2].split()) # Convert to bag of words format first\n",
    "    # Get the topics and words associated with each document\n",
    "    docTopics, wordTopics, phiValues = lda.get_document_topics(doc, per_word_topics=True)\n",
    "    topNineFile.write(yearOfAbstract[x]+\"\\t\"+abst+\"\\t\")\n",
    "    for z in xrange(0,len(phiValues)):\n",
    "        phiValues[z][1].sort(key=lambda q:q[1],reverse=True)\n",
    "    phiValues.sort(key=lambda q:q[1][0][1],reverse=True)\n",
    "    nineWords = \"\"\n",
    "    for x in phiValues[:15]:\n",
    "        nineWords+= dictionary[x[0]] + \" \"\n",
    "    topNineFile.write(nineWords.encode('utf-8')+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = api.flaten(yearlyCorpora[0])\n",
    "first.sort(key=lambda x: x[0])\n",
    "print api.binarySearch(first,0,lambda x: x[0])\n",
    "print first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = occurences.values()\n",
    "vals.sort(key=lambda x:x['occurences'],reverse=True)\n",
    "print vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedVals = sorted(occurences.values(),key=lambda k:k[\"tag\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50 = keyphraseTracker.topN()\n",
    "keyphraseNodes = {}\n",
    "for yo in top50:\n",
    "    keyphraseNodes[yo.getPhrase()] = yo.sum()\n",
    "keyphraseEdges = []\n",
    "for yo in top50:\n",
    "    for (phr,occur) in yo.cooccurringPhrases.iteritems():\n",
    "        #rint (phr,occur)\n",
    "        if phr in keyphraseNodes.keys():\n",
    "            keyphraseEdges.append((yo.getPhrase(),phr,occur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print len(keyphraseEdges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'api mellifera', 'nectar', 10)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphraseMapper = api.WordMapper()\n",
    "with open(\"../data/documents/GephiFiles/KeyWords/nodes.csv\",\"w\") as nodesFile:\n",
    "    nodesFile.write(\"Id,Label,Weight\\n\")\n",
    "    for (phr,occur) in keyphraseNodes.iteritems():\n",
    "        id = keyphraseMapper.mapWord(phr)\n",
    "        nodesFile.write(str(id)+\",\"+phr+\",\"+str(occur)+\"\\n\")\n",
    "mappedEdges = [(keyphraseMapper.mapWord(t[0]),keyphraseMapper.mapWord(t[1]),t[2]) for t in keyphraseEdges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/documents/GephiFiles/KeyWords/edges.csv\",\"w\") as edgeFile:\n",
    "    edgeFile.write(\"Source,Destination,Weight\\n\")\n",
    "    for (id1,id2,w) in mappedEdges:\n",
    "        edgeFile.write(str(id1)+\",\"+str(id2)+\",\"+str(w)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-427a2dfbe4a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselection\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "selection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2006: 96}\n",
      "{2006: 51}\n",
      "{2006: 40}\n",
      "{2006: 22}\n",
      "{2006: 18}\n",
      "{2006: 16}\n",
      "{2006: 11}\n",
      "{2006: 10}\n",
      "{2006: 9}\n",
      "{2006: 8}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 7}\n",
      "{2006: 6}\n",
      "{2006: 6}\n",
      "{2006: 6}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 5}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 4}\n",
      "{2006: 3}\n"
     ]
    }
   ],
   "source": [
    "w = keyphraseTracker.topN()\n",
    "for wr in w:\n",
    "    print wr.years"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
